ABLATION STUDY: age_campaign Feature Value Analysis
============================================================

Configuration:
  - Model: LightGBM with SMOTE
  - Cross-Validation: 5-fold Stratified
  - Random State: 42

Results:
------------------------------------------------------------
Model                          | Mean AUC     | Std       
------------------------------------------------------------
WITH age_campaign              | 0.7822       | 0.0050    
WITHOUT age_campaign           | 0.7807       | 0.0051    
------------------------------------------------------------

Difference: +0.0015 (+0.19%)

Fold-by-Fold Comparison:
Fold       | With         | Without      | Diff        
--------------------------------------------------
Fold 1     | 0.7800       | 0.7790       | +0.0010
Fold 2     | 0.7800       | 0.7770       | +0.0030
Fold 3     | 0.7914       | 0.7893       | +0.0021
Fold 4     | 0.7830       | 0.7835       | -0.0005
Fold 5     | 0.7767       | 0.7749       | +0.0017

Conclusion: POSITIVE IMPACT: The age_campaign feature improves AUC by 0.0015.


The results are saved in 
age_campaign_ablation_results.txt
. Here's the summary:

Model	Mean AUC	Std
WITH age_campaign	0.7822	0.0050
WITHOUT age_campaign	0.7807	0.0051
Difference: +0.0015 (+0.19%)

Key Findings
The feature has a marginal positive impact — but only +0.15% improvement in AUC
Inconsistent across folds — In Fold 4, the model actually performed worse with the feature (-0.0005)
Within standard deviation — The improvement (0.0015) is smaller than the standard deviation (0.005), meaning it's likely not statistically significant
Verdict
While technically positive, the age_campaign feature provides negligible value. The improvement is:

Smaller than fold-to-fold variance
Not robust (1 out of 5 folds showed negative impact)
Likely within the margin of noise
For a production model, you could safely remove this feature to reduce complexity without meaningful performance loss.



Key Finding
Surprisingly, is_first_contact provides even less value than age_campaign!

This confirms my earlier point: LightGBM can learn the pdays == 999 split directly from the raw pdays feature. The explicit binary encoding adds no information the model couldn't discover on its own.

Takeaway
Both engineered features are redundant for tree-based models. The model already has access to:

age and campaign separately (can learn any interaction)
pdays directly (can split at 999)
These features would be more valuable for linear models (logistic regression, SVM) that cannot learn interactions or thresholds automatically. For LightGBM, they add complexity without meaningful benefit.